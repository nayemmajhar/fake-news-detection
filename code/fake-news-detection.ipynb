{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_news = pd.read_csv('./data/Fake.csv')\n",
    "true_news = pd.read_csv('./data/True.csv')\n",
    "\n",
    "fake_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label dataset\n",
    "fake_news['label'] = 0\n",
    "true_news['label'] = 1\n",
    "\n",
    "# concat two dataset\n",
    "df = pd.concat([fake_news,true_news], axis=0)\n",
    "\n",
    "# drop 'subject' column\n",
    "df.drop('subject', axis=1, inplace=True)\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df['text'] = df['title'] + ' ' + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Turkish foreign minister says joint operation ...</td>\n",
       "      <td>Turkish foreign minister says joint operation ...</td>\n",
       "      <td>September 26, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bush Once Apologized To China In Order To Fre...</td>\n",
       "      <td>Bush Once Apologized To China In Order To Fre...</td>\n",
       "      <td>January 15, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOL! NEW VIDEO Emerges Of Central Park Trump A...</td>\n",
       "      <td>LOL! NEW VIDEO Emerges Of Central Park Trump A...</td>\n",
       "      <td>Jul 6, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump says Mexican imports tax one option but ...</td>\n",
       "      <td>Trump says Mexican imports tax one option but ...</td>\n",
       "      <td>January 27, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madeleine Albright: Trump Needs To Stay The F...</td>\n",
       "      <td>Madeleine Albright: Trump Needs To Stay The F...</td>\n",
       "      <td>May 2, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Turkish foreign minister says joint operation ...   \n",
       "1   Bush Once Apologized To China In Order To Fre...   \n",
       "2  LOL! NEW VIDEO Emerges Of Central Park Trump A...   \n",
       "3  Trump says Mexican imports tax one option but ...   \n",
       "4   Madeleine Albright: Trump Needs To Stay The F...   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  Turkish foreign minister says joint operation ...  September 26, 2017    \n",
       "1   Bush Once Apologized To China In Order To Fre...     January 15, 2016   \n",
       "2  LOL! NEW VIDEO Emerges Of Central Park Trump A...          Jul 6, 2017   \n",
       "3  Trump says Mexican imports tax one option but ...    January 27, 2017    \n",
       "4   Madeleine Albright: Trump Needs To Stay The F...          May 2, 2017   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      0  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of stopwords for english\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.stem.porter.PorterStemmer()\n",
    "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def delete_punctuation(text):\n",
    "    chars = []\n",
    "    for char in text:\n",
    "        if char not in string.punctuation:\n",
    "            chars.append(char)\n",
    "        else:\n",
    "            chars.append(' ')\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleaning_and_processing_text(txt):\n",
    "    \n",
    "    # lower cased the text\n",
    "    text = re.sub('[^a-zA-Z]',' ',txt)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('\\\\W', ' ', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub(' $', '', text)\n",
    "    text = delete_punctuation(text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove stopwords, Stemming (remove -ing, -ly, ...) and Lemmatisation\n",
    "    text = text.split()\n",
    "    clean_text = [lem.lemmatize(word) for word in text if word not in stopwords]\n",
    "    clean_text = [ps.stem(word) for word in clean_text]\n",
    "    \n",
    "    text = \" \".join(clean_text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(lambda x: cleaning_and_processing_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Turkish foreign minister says joint operation ...</td>\n",
       "      <td>Turkish foreign minister says joint operation ...</td>\n",
       "      <td>September 26, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>turkish foreign minist say joint oper iraq tab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bush Once Apologized To China In Order To Fre...</td>\n",
       "      <td>Bush Once Apologized To China In Order To Fre...</td>\n",
       "      <td>January 15, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>bush apolog china order free detain u soldier ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOL! NEW VIDEO Emerges Of Central Park Trump A...</td>\n",
       "      <td>LOL! NEW VIDEO Emerges Of Central Park Trump A...</td>\n",
       "      <td>Jul 6, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>lol new video emerg central park trump assassi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump says Mexican imports tax one option but ...</td>\n",
       "      <td>Trump says Mexican imports tax one option but ...</td>\n",
       "      <td>January 27, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>trump say mexican import tax one option other ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madeleine Albright: Trump Needs To Stay The F...</td>\n",
       "      <td>Madeleine Albright: Trump Needs To Stay The F...</td>\n",
       "      <td>May 2, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>madelein albright trump need stay f ck away ki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Turkish foreign minister says joint operation ...   \n",
       "1   Bush Once Apologized To China In Order To Fre...   \n",
       "2  LOL! NEW VIDEO Emerges Of Central Park Trump A...   \n",
       "3  Trump says Mexican imports tax one option but ...   \n",
       "4   Madeleine Albright: Trump Needs To Stay The F...   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  Turkish foreign minister says joint operation ...  September 26, 2017    \n",
       "1   Bush Once Apologized To China In Order To Fre...     January 15, 2016   \n",
       "2  LOL! NEW VIDEO Emerges Of Central Park Trump A...          Jul 6, 2017   \n",
       "3  Trump says Mexican imports tax one option but ...    January 27, 2017    \n",
       "4   Madeleine Albright: Trump Needs To Stay The F...          May 2, 2017   \n",
       "\n",
       "   label                                         clean_text  \n",
       "0      1  turkish foreign minist say joint oper iraq tab...  \n",
       "1      0  bush apolog china order free detain u soldier ...  \n",
       "2      0  lol new video emerg central park trump assassi...  \n",
       "3      1  trump say mexican import tax one option other ...  \n",
       "4      0  madelein albright trump need stay f ck away ki...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['turkish foreign minist say joint oper iraq tabl referendum ankara reuter turkey ass request made iraqi central govern wake iraqi kurdish independ referendum includ joint oper iraq turkish foreign minist mevlut cavusoglu said tuesday evalu request iraq everyth includ joint oper tabl said interview broadcast kanal ad reason turkey close habur border gate iraq'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'][:1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse feature matrix from text using TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english') # params: max_features=500 can be used\n",
    "vector_df = vectorizer.fit_transform(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44898, 89469)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important features using TruncatedSVD\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=1000, n_iter=7)\n",
    "truncated_x = pd.DataFrame(svd.fit_transform(vector_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.091031</td>\n",
       "      <td>0.132482</td>\n",
       "      <td>-0.029735</td>\n",
       "      <td>-0.072650</td>\n",
       "      <td>0.037826</td>\n",
       "      <td>-0.069727</td>\n",
       "      <td>0.093701</td>\n",
       "      <td>0.017928</td>\n",
       "      <td>-0.107694</td>\n",
       "      <td>0.011955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022111</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>-0.009884</td>\n",
       "      <td>0.009920</td>\n",
       "      <td>0.021859</td>\n",
       "      <td>0.015478</td>\n",
       "      <td>-0.002446</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>-0.009024</td>\n",
       "      <td>-0.011870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.209207</td>\n",
       "      <td>0.085917</td>\n",
       "      <td>0.021894</td>\n",
       "      <td>-0.049114</td>\n",
       "      <td>-0.055749</td>\n",
       "      <td>0.036453</td>\n",
       "      <td>-0.027331</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>-0.025145</td>\n",
       "      <td>0.024601</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002890</td>\n",
       "      <td>-0.004037</td>\n",
       "      <td>-0.018505</td>\n",
       "      <td>-0.020299</td>\n",
       "      <td>-0.003295</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>-0.015787</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>-0.014022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.165406</td>\n",
       "      <td>-0.095752</td>\n",
       "      <td>0.082599</td>\n",
       "      <td>-0.041463</td>\n",
       "      <td>-0.061021</td>\n",
       "      <td>-0.035573</td>\n",
       "      <td>-0.032436</td>\n",
       "      <td>0.027905</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>-0.030353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>-0.010542</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>-0.003340</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>0.022158</td>\n",
       "      <td>-0.003485</td>\n",
       "      <td>-0.015867</td>\n",
       "      <td>-0.001940</td>\n",
       "      <td>-0.010859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270206</td>\n",
       "      <td>-0.047351</td>\n",
       "      <td>0.077081</td>\n",
       "      <td>0.097433</td>\n",
       "      <td>-0.099924</td>\n",
       "      <td>-0.100079</td>\n",
       "      <td>0.074297</td>\n",
       "      <td>0.096302</td>\n",
       "      <td>0.028359</td>\n",
       "      <td>0.109901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014890</td>\n",
       "      <td>-0.001317</td>\n",
       "      <td>-0.003443</td>\n",
       "      <td>-0.033675</td>\n",
       "      <td>0.012974</td>\n",
       "      <td>-0.008207</td>\n",
       "      <td>0.046135</td>\n",
       "      <td>0.004722</td>\n",
       "      <td>-0.016815</td>\n",
       "      <td>-0.028489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.212460</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>0.157728</td>\n",
       "      <td>0.033680</td>\n",
       "      <td>-0.083061</td>\n",
       "      <td>0.027632</td>\n",
       "      <td>-0.012635</td>\n",
       "      <td>-0.003381</td>\n",
       "      <td>0.006598</td>\n",
       "      <td>-0.010978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>-0.003674</td>\n",
       "      <td>0.008843</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.011260</td>\n",
       "      <td>0.009747</td>\n",
       "      <td>0.006747</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>-0.014397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>0.073633</td>\n",
       "      <td>0.040172</td>\n",
       "      <td>0.005879</td>\n",
       "      <td>-0.019897</td>\n",
       "      <td>0.039950</td>\n",
       "      <td>-0.015253</td>\n",
       "      <td>-0.011610</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>-0.013124</td>\n",
       "      <td>0.032894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003643</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>-0.006583</td>\n",
       "      <td>0.020252</td>\n",
       "      <td>0.019157</td>\n",
       "      <td>-0.016508</td>\n",
       "      <td>-0.015691</td>\n",
       "      <td>-0.002139</td>\n",
       "      <td>-0.004252</td>\n",
       "      <td>-0.005073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>0.153520</td>\n",
       "      <td>-0.076832</td>\n",
       "      <td>-0.015291</td>\n",
       "      <td>-0.073766</td>\n",
       "      <td>0.311429</td>\n",
       "      <td>0.156731</td>\n",
       "      <td>-0.077113</td>\n",
       "      <td>0.080659</td>\n",
       "      <td>0.108633</td>\n",
       "      <td>0.086829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>-0.015648</td>\n",
       "      <td>0.006894</td>\n",
       "      <td>-0.002009</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>-0.004356</td>\n",
       "      <td>-0.015263</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.022967</td>\n",
       "      <td>0.019397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>0.368518</td>\n",
       "      <td>-0.234429</td>\n",
       "      <td>0.069732</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.051450</td>\n",
       "      <td>0.191439</td>\n",
       "      <td>0.210409</td>\n",
       "      <td>0.022763</td>\n",
       "      <td>-0.085624</td>\n",
       "      <td>0.112158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004880</td>\n",
       "      <td>-0.001634</td>\n",
       "      <td>-0.021154</td>\n",
       "      <td>-0.001816</td>\n",
       "      <td>-0.010452</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>0.009101</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>0.011659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>0.162447</td>\n",
       "      <td>-0.016473</td>\n",
       "      <td>-0.042122</td>\n",
       "      <td>-0.045544</td>\n",
       "      <td>-0.060031</td>\n",
       "      <td>0.031685</td>\n",
       "      <td>-0.035133</td>\n",
       "      <td>0.013472</td>\n",
       "      <td>-0.003036</td>\n",
       "      <td>-0.046678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012457</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.009053</td>\n",
       "      <td>0.012590</td>\n",
       "      <td>-0.010859</td>\n",
       "      <td>-0.022344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>0.133166</td>\n",
       "      <td>-0.028523</td>\n",
       "      <td>0.041377</td>\n",
       "      <td>0.012385</td>\n",
       "      <td>-0.047887</td>\n",
       "      <td>-0.050018</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.007110</td>\n",
       "      <td>0.018076</td>\n",
       "      <td>0.037863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>-0.010640</td>\n",
       "      <td>-0.008935</td>\n",
       "      <td>-0.008719</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>-0.017579</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.003693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.091031  0.132482 -0.029735 -0.072650  0.037826 -0.069727  0.093701   \n",
       "1      0.209207  0.085917  0.021894 -0.049114 -0.055749  0.036453 -0.027331   \n",
       "2      0.165406 -0.095752  0.082599 -0.041463 -0.061021 -0.035573 -0.032436   \n",
       "3      0.270206 -0.047351  0.077081  0.097433 -0.099924 -0.100079  0.074297   \n",
       "4      0.212460  0.012887  0.157728  0.033680 -0.083061  0.027632 -0.012635   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "44893  0.073633  0.040172  0.005879 -0.019897  0.039950 -0.015253 -0.011610   \n",
       "44894  0.153520 -0.076832 -0.015291 -0.073766  0.311429  0.156731 -0.077113   \n",
       "44895  0.368518 -0.234429  0.069732  0.008039  0.051450  0.191439  0.210409   \n",
       "44896  0.162447 -0.016473 -0.042122 -0.045544 -0.060031  0.031685 -0.035133   \n",
       "44897  0.133166 -0.028523  0.041377  0.012385 -0.047887 -0.050018  0.024561   \n",
       "\n",
       "            7         8         9    ...       990       991       992  \\\n",
       "0      0.017928 -0.107694  0.011955  ...  0.022111  0.017383 -0.009884   \n",
       "1      0.004205 -0.025145  0.024601  ... -0.002890 -0.004037 -0.018505   \n",
       "2      0.027905  0.017745 -0.030353  ...  0.004260 -0.010542  0.006757   \n",
       "3      0.096302  0.028359  0.109901  ... -0.014890 -0.001317 -0.003443   \n",
       "4     -0.003381  0.006598 -0.010978  ...  0.014134  0.014665 -0.003674   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "44893  0.011194 -0.013124  0.032894  ... -0.003643  0.003609 -0.006583   \n",
       "44894  0.080659  0.108633  0.086829  ...  0.003137 -0.015648  0.006894   \n",
       "44895  0.022763 -0.085624  0.112158  ... -0.004880 -0.001634 -0.021154   \n",
       "44896  0.013472 -0.003036 -0.046678  ...  0.012457  0.001782  0.009167   \n",
       "44897  0.007110  0.018076  0.037863  ...  0.002585 -0.010640 -0.008935   \n",
       "\n",
       "            993       994       995       996       997       998       999  \n",
       "0      0.009920  0.021859  0.015478 -0.002446  0.015010 -0.009024 -0.011870  \n",
       "1     -0.020299 -0.003295  0.003077 -0.015787  0.002142  0.010265 -0.014022  \n",
       "2     -0.003340  0.005709  0.022158 -0.003485 -0.015867 -0.001940 -0.010859  \n",
       "3     -0.033675  0.012974 -0.008207  0.046135  0.004722 -0.016815 -0.028489  \n",
       "4      0.008843  0.000285  0.011260  0.009747  0.006747  0.007141 -0.014397  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "44893  0.020252  0.019157 -0.016508 -0.015691 -0.002139 -0.004252 -0.005073  \n",
       "44894 -0.002009  0.021484 -0.004356 -0.015263  0.000187 -0.022967  0.019397  \n",
       "44895 -0.001816 -0.010452  0.000137  0.015298  0.009101  0.002958  0.011659  \n",
       "44896  0.001312  0.002301  0.009072  0.009053  0.012590 -0.010859 -0.022344  \n",
       "44897 -0.008719  0.022081  0.011503  0.004295 -0.017579 -0.007368 -0.003693  \n",
       "\n",
       "[44898 rows x 1000 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_train shape (1, 35918). \n",
      "\n",
      "y_train type <class 'numpy.ndarray'>. \n",
      "\n",
      "y_train dimension 2. \n",
      "\n",
      "X_train shape (35918, 1000). \n",
      "\n",
      "X_train type <class 'pandas.core.frame.DataFrame'>. \n",
      "\n",
      "X_train dimension 2. \n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(truncated_x, df['label'], test_size=0.20, random_state=10)\n",
    "\n",
    "# display(X_train.head())\n",
    "# print('\\n')\n",
    "# display(y_train.head())\n",
    "y_train = y_train.values.reshape([y_train.shape[0], -1]).T\n",
    "# X_train_flatten = X_train.values.reshape([X_train.shape[0], -1]).T\n",
    "# X_train = X_train_flatten/255\n",
    "# y_test = pd.DataFrame(y_test.values.reshape(y_test.shape[0], -1).T)\n",
    "print(\"\\ny_train shape {}. \".format(y_train.shape))\n",
    "print(\"\\ny_train type {}. \".format(type(y_train)))\n",
    "print(\"\\ny_train dimension {}. \".format(y_train.ndim))\n",
    "print(\"\\nX_train shape {}. \".format(X_train.shape))\n",
    "print(\"\\nX_train type {}. \".format(type(X_train)))\n",
    "print(\"\\nX_train dimension {}. \".format(X_train.ndim))\n",
    "\n",
    "# print(\"\\nThere are {} documents in the training data.\".format(len(X_train)))\n",
    "# print(\"\\nThere are {} documents in the test data.\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky Relu implementation\n",
    "def leaky_relu(Z):\n",
    "    A = np.where(Z > 0, Z, Z * 0.01)\n",
    "\n",
    "# Faster method than the above but will test later\n",
    "#     A1 = ((Z>0) * Z)\n",
    "#     A2 = ((Z<=0) * Z * 0.01)\n",
    "#     A = A1 + A2\n",
    "\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def leaky_relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    " \n",
    "    dZ[Z <= 0] = 0.01\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "# Sigmoid \n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "# Relu\n",
    "def relu(Z):\n",
    "#     print(\"\\nshape of Z is {}\".format(Z.shape))\n",
    "#     print(\"type of Z: \" + str(type(Z)))\n",
    "#     print(\"dimension of Z: \" + str(Z.ndim))\n",
    "    A = np.maximum(0,Z)\n",
    "\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Parameters W and b\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for layer in range(1, L):\n",
    "        parameters[\"W\" + str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer-1]) * 0.01\n",
    "        parameters[\"b\" + str(layer)] = np.zeros((layer_dims[layer], 1))\n",
    "        \n",
    "        assert(parameters[\"W\" + str(layer)].shape == (layer_dims[layer], layer_dims[layer-1]))\n",
    "        assert(parameters[\"b\" + str(layer)].shape == (layer_dims[layer], 1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "\n",
    "# Forward Propagation with Linear function, activation functions\n",
    "\n",
    "def linear_func_forward(A, W, b):\n",
    "#     print(\"\\nshape of A is {}\".format(A.shape))\n",
    "#     print(\"type of A: \" + str(type(A)))\n",
    "#     print(\"dimension of A: \" + str(A.ndim))\n",
    "#     print(\"\\nshape of W is {}\".format(W.shape))\n",
    "#     print(\"type of W: \" + str(type(W)))\n",
    "#     print(\"dimension of W: \" + str(W.ndim))\n",
    "    Z= np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "## Activation functions Sigmoid/leaky_Relu based on the layers\n",
    "def activation_func(A_prev, W, b, actviation_choice):\n",
    "\n",
    "    if actviation_choice == \"sigmoid\":\n",
    "        Z, linear_cache = linear_func_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif actviation_choice == \"relu\":\n",
    "        Z, linear_cache = linear_func_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif actviation_choice == \"leaky_relu\":\n",
    "        Z, linear_cache = linear_func_forward(A_prev, W, b)\n",
    "        A, activation_cache = leaky_relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    print(\"\\nnumber of layers is {}\".format(L))\n",
    "    \n",
    "    for layer in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = activation_func(A_prev, parameters['W'+str(layer)], parameters['b'+str(layer)], actviation_choice=\"relu\")\n",
    "        caches.append(cache)\n",
    "#         print(\"\\nshape of A is {}\".format(A.shape))\n",
    "    \n",
    "    AL, cache = activation_func(A, parameters['W'+str(L)], parameters['b'+str(L)], actviation_choice=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches\n",
    "\n",
    "\n",
    "# Computing cost after the forward propagtion\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "#     print(\"\\ntype of AL {}\".format(type(AL)))\n",
    "#     print(\"\\ntype of Y {}\".format(type(Y)))\n",
    "    cost = -1/m* np.sum((np.dot(Y, np.log(AL).T)) + (np.dot((1-Y), np.log(1-AL).T)))\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# Backward Propagation with linear function, activation functions\n",
    "def linear_func_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def actvtion_func_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_func_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_func_backward(dZ, linear_cache)\n",
    "    \n",
    "    elif activation == \"leaky_relu\":\n",
    "        dZ = leaky_relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_func_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def backward_propagation(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = actvtion_func_backward(dAL, current_cache, \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = actvtion_func_backward(grads[\"dA\" + str(l+1)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "    \n",
    "    return grads\n",
    "\n",
    "# After backward propagation update parameters function is called\n",
    "def update_parameters(params, grads, learning_rate):\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_costs(costs, learning_rate=0.0075):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [35918, 40, 20, 10, 5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(X, Y, layer_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost = False):\n",
    "    np.random.seed(1)\n",
    "    costs=[]\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        print(\"\\nshape of AL is {}\".format(AL.shape))\n",
    "        print(\"type of AL: \" + str(type(AL)))\n",
    "        print(\"dimension of AL: \" + str(AL.ndim))\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of layers is 5\n",
      "\n",
      "shape of AL is (1, 1000)\n",
      "type of AL: <class 'numpy.ndarray'>\n",
      "dimension of AL: 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,35918) and (1000,1) not aligned: 35918 (dim 1) != 1000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-350-456fe6e27493>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-349-e3ccb8d4b2bc>\u001b[0m in \u001b[0;36mtraining_model\u001b[1;34m(X, Y, layer_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dimension of AL: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-346-9f908acce7fb>\u001b[0m in \u001b[0;36mcompute_cost\u001b[1;34m(AL, Y)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m#     print(\"\\ntype of AL {}\".format(type(AL)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;31m#     print(\"\\ntype of Y {}\".format(type(Y)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,35918) and (1000,1) not aligned: 35918 (dim 1) != 1000 (dim 0)"
     ]
    }
   ],
   "source": [
    "parameters, costs = training_model(X_train, y_train, layers_dims, num_iterations = 200, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 35918)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(layers_dims)\n",
    "# parameters[\"b1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
